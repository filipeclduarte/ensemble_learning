{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistemas de Múltiplos Classificadores\n",
    "\n",
    "## 1)\n",
    "\n",
    "Selecione cinco bases de dados públicas que contenham características diferentes e, para cada base, calcule o Oracle no conjunto de teste para:\n",
    "* Bagging\n",
    "* Adaboost \n",
    "* Random Subspace (50%)\n",
    "* Random Oracles\n",
    "\n",
    "variando o número de classificadores-base {10, 20, …, 100}. \n",
    "\n",
    "Use o Perceptron como classificador-base e divida os fold usando o 5-fold cross-validation. \n",
    "\n",
    "Analise os resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n",
    "\n",
    "4. \n",
    "\n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier # servirá para o Bagging e para o Random Subspace\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaseEnsemble\n",
    "from rlo import RLO\n",
    "\n",
    "import oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "np.random.seed(111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementação do RLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLO(BaseEnsemble):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=10,\n",
    "                 E = None,\n",
    "                 H = None,\n",
    "                ):\n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators)\n",
    "        \n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        N = X.shape[0]\n",
    "        self.E = {f'D_{i}':{'left:':None, 'right':None} for i in range(self.n_estimators)}\n",
    "        self.H = {f'H_{i}':None for i in range(self.n_estimators)}\n",
    "        \n",
    "        for i in range(self.n_estimators):     \n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    rp = np.random.choice(N, 2)\n",
    "                    A, B = X[rp[0], :], X[rp[1], :]\n",
    "                    self.H[f'H_{i}'] = np.hstack((A - B, (B@B.T - A@A.T)/2))\n",
    "                    left = np.hstack((X, np.ones((N, 1)))) @ self.H[f'H_{i}'].T > 0\n",
    "                    classificador_left = self.base_estimator\n",
    "                    classificador_right = self.base_estimator\n",
    "                    self.E[f'D_{i}']['left'] = classificador_left.fit(X[left, :], y[left])\n",
    "                    self.E[f'D_{i}']['right'] = classificador_right.fit(X[~left, :], y[~left])\n",
    "                    \n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "                break\n",
    "                \n",
    "\n",
    "        return self.E, self.H\n",
    "#         return (E, H)\n",
    "\n",
    "    def predict(self, X, y):\n",
    "\n",
    "        L = len(self.E)\n",
    "        ens = np.zeros((X.shape[0], L))\n",
    "\n",
    "        for i in range(L):\n",
    "            left = np.hstack((X, np.ones((X.shape[0], 1)))) @ self.H[f'H_{i}'].T > 0\n",
    "            ens[left, i] = self.E[f'D_{i}']['left'].predict(X[left, :])\n",
    "            ens[~left, i] = self.E[f'D_{i}']['right'].predict(X[~left, :])\n",
    "\n",
    "        a1 = stats.mode(ens.T)[0]\n",
    "        e = np.mean(a1 != y)\n",
    "\n",
    "        return (ens, a1, e)\n",
    "    \n",
    "    def Oracle_predict(self, X, y):\n",
    "        predictions, _, _ = self.predict(X, y)\n",
    "        pred_oracle = np.any((predictions == y.reshape(-1,1)), axis=1)\n",
    "        return pred_oracle\n",
    "    \n",
    "    def Oracle_score(self, X, y):\n",
    "        pred_oracle = self.Oracle_predict(X, y)\n",
    "        acc_oracle = pred_oracle.sum() / pred_oracle.shape[0]\n",
    "        return acc_oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Implementação do Random Linear Oracle (RLO) baseado no livro de Kuncheva\n",
    "\n",
    "\n",
    "\n",
    "# def rlo_train(X, y, L, modelo):\n",
    "#     N = X.shape[0]\n",
    "#     E = {f'D_{i}':{'left:':None, 'right':None} for i in range(L)}\n",
    "#     H = {f'H_{i}':None for i in range(L)}\n",
    "    \n",
    "#     for i in range(L):\n",
    "#         rp = np.random.choice(N, 2)\n",
    "#         A, B = X[rp[0], :], X[rp[1], :]\n",
    "#         H[f'H_{i}'] = np.hstack((A - B, (B@B.T - A@A.T)/2))\n",
    "#         left = np.hstack((X, np.ones((N, 1)))) @ H[f'H_{i}'].T > 0\n",
    "#         classificador_left = modelo\n",
    "#         classificador_right = modelo\n",
    "#         E[f'D_{i}']['left'] = classificador_left.fit(X[left, :], y[left])\n",
    "#         E[f'D_{i}']['right'] = classificador_right.fit(X[~left, :], y[~left])\n",
    "\n",
    "#     return (E, H)\n",
    "\n",
    "# def rlo_classificar(E, H, X, y):\n",
    "    \n",
    "#     L = len(E)\n",
    "#     ens = np.zeros((X.shape[0], L))\n",
    "    \n",
    "#     for i in range(L):\n",
    "#         left = np.hstack((X, np.ones((X.shape[0], 1)))) @ H[f'H_{i}'].T > 0\n",
    "#         ens[left, i] = E[f'D_{i}']['left'].predict(X[left, :])\n",
    "#         ens[~left, i] = E[f'D_{i}']['right'].predict(X[~left, :])\n",
    "    \n",
    "#     a1 = stats.mode(ens.T)[0]\n",
    "#     e = np.mean(a1 != y)\n",
    "    \n",
    "#     return (a1, e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de classificadores base\n",
    "n_classificadores_base = np.arange(10, 110, 10)\n",
    "print(n_classificadores_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base de dados para teste\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "# split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Scale the variables to have 0 mean and unit variance\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)\n",
    "\n",
    "\n",
    "# # Split the data into training and DSEL for DS techniques\n",
    "# X_train, X_dsel, y_train, y_dsel = train_test_split(X_train, y_train, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando o RLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_base = Perceptron(max_iter=100)\n",
    "rlo = RLO(base_estimator=cls_base, n_estimators=3)\n",
    "\n",
    "# treinar\n",
    "rlo.fit(X_train, y_train)\n",
    "\n",
    "# testar \n",
    "predictions, pred_ens, erro = rlo.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erro_rlo = np.mean(pred_ens == y_test)\n",
    "print(erro_rlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle\n",
    "# pred_oracle = np.any((predictions == y_dsel.reshape(-1,1)), axis=1)\n",
    "# acc_oracle = pred_oracle.sum()/pred_oracle.shape[0]\n",
    "# print(f'acc Oracle: {acc_oracle}')\n",
    "pred_oracle = rlo.Oracle_predict(X_test, y_test)\n",
    "acc_oracle = rlo.Oracle_score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Oracle Accuracy: {acc_oracle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### função que faz a Validação cruzada 5fold com o RLO\n",
    "#### função avalia_L_modelos para o RLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os 5 folds e salvando em dicionário\n",
    "def cria_folds(X, y, n=5):\n",
    "    kf = StratifiedKFold(n_splits=n)\n",
    "    kfolds = dict()\n",
    "    i = 1\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        kfolds[f'fold{i}'] = (train_index, test_index)\n",
    "        i += 1\n",
    "    return kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando os 5-folds\n",
    "kfolds = cria_folds(X_train,y_train,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## função cv RLO\n",
    "def cv_RLO(kfolds, RLO, X, y):\n",
    "    '''\n",
    "    Realiza a validação cruzada para um Random Linear Oracle.\n",
    "    \n",
    "    Retorna uma tupla com dois dicionários: modelos e resultados.\n",
    "    '''\n",
    "    modelos = dict.fromkeys(kfolds)\n",
    "    resultados = dict.fromkeys(kfolds)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in kfolds.items():\n",
    "        modelo = RLO\n",
    "        _,_ = modelo.fit(X[train_idx], y[train_idx])\n",
    "        modelos[fold] = modelo\n",
    "#         resultados[fold] = modelo.predict(X[test_idx], y[test_idx])\n",
    "        resultados[fold] = modelo.predict(X[train_idx], y[train_idx])\n",
    "    \n",
    "    return (modelos, resultados)\n",
    "    \n",
    "\n",
    "## função avalia_L_modelos_RLO\n",
    "def avalia_cv_RLO(kfolds, modelos_cv, X, y):\n",
    "    '''\n",
    "    Realiza a avaliação dos resultados para os dados de teste para o Random Linear Oracle e computa o Oracle para o conjunto de teste\n",
    "    \n",
    "    Retorna uma tupla com dois dicionários: resultados, oracles\n",
    "    '''\n",
    "    \n",
    "    resultados = dict.fromkeys(modelos_cv)\n",
    "    oracles = dict.fromkeys(modelos_cv)\n",
    "    for fold, (_, test_idx) in kfolds.items():\n",
    "        resultados[fold] = modelos_cv[fold].predict(X[test_idx], y[test_idx])\n",
    "        oracles[fold] = modelos_cv[fold].Oracle_score(X[test_idx], y[test_idx])\n",
    "    return (resultados, oracles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos_cv, resultados_treinamento_cv = cv_RLO(kfolds, RLO(base_estimator=Perceptron(max_iter=10)), X_train, y_train)\n",
    "resultados_teste_cv, oracles_cv = avalia_cv_RLO(kfolds, modelos_cv, X_train, y_train)\n",
    "\n",
    "# modelos_cv, resultados_treinamento_cv = cv_RLO(kfolds, RLO(base_estimator=Perceptron(max_iter=100)), X, y)\n",
    "# resultados_teste_cv, oracles_cv = avalia_cv_RLO(kfolds, modelos_cv, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mostrando Oracle para cada fold\n",
    "print(\"Acurácia do Oracle para cada Fold:\")\n",
    "oracles_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Realizando o procedimento anterior variando agora a quantidade de classificadores do modelo\n",
    "\n",
    "### quantidade de classificadores\n",
    "n_classificadores = np.arange(10, 110, 10)\n",
    "\n",
    "## criando dicionário par receber os resultados para treinamento e teste\n",
    "rlo_classificadores_resultados_treinamento = dict.fromkeys(n_classificadores)\n",
    "rlo_classificadores_resultados_teste = dict.fromkeys(n_classificadores)\n",
    "\n",
    "## Loop para cada n dos L classificadores\n",
    "for n in n_classificadores:\n",
    "    print('Qtd. classificadores: ', n)\n",
    "    ## treinamento\n",
    "    while True:\n",
    "        try:\n",
    "            modelos, resultados = cv_RLO(kfolds,RLO(base_estimator=Perceptron(max_iter=5), n_estimators=n), X_train, y_train)\n",
    "            rlo_classificadores_resultados_treinamento[n] = (modelos, resultados)\n",
    "            ## avalia cv \n",
    "            resultados_teste, oracles_teste = avalia_cv_RLO(kfolds, modelos, X_train, y_train)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "        break\n",
    "            \n",
    "    rlo_classificadores_resultados_teste[n] = (resultados_teste, oracles_teste)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avaliando Oracle para cada n\n",
    "\n",
    "rlo_oracles = dict.fromkeys(n_classificadores)\n",
    "for n in n_classificadores:\n",
    "    oracles_n = []\n",
    "    for i in range(1,6):\n",
    "        fold = f'fold{i}'\n",
    "        oracle_i = rlo_classificadores_resultados_teste[n][1][fold]\n",
    "        oracles_n.append(oracle_i)\n",
    "    rlo_oracles[n] = oracles_n\n",
    "# for n in n_classificadores:\n",
    "#     print(rlo_classificadores_resultados_teste[n][1]['fold2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformando em DataFrame\n",
    "rlo_oracle_df = pd.DataFrame(rlo_oracles)\n",
    "## Calculando estatísticas\n",
    "rlo_oracle_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlo_oracle_df.boxplot(figsize=(10,8), grid=False)\n",
    "plt.title('Boxplot: Oracle para o RLO no conjunto de teste')\n",
    "plt.xlabel('Quantidade de classificadores')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.boxplot(rlo_oracles.values(), labels=rlo_oracles.keys())\n",
    "plt.title('Boxplot: Oracle para o RLO no conjunto de teste')\n",
    "plt.xlabel('Quantidade de classificadores')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar funções para os demais modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para criar os classificadores com o Perceptron como classificador base\n",
    "def L_modelos(classificador, random_subspace=False):\n",
    "    if random_subspace:\n",
    "        max_features = 0.5 # 50% de features\n",
    "        modelos = dict()\n",
    "        n_classificadores = np.arange(10, 110, 10)\n",
    "        for n in n_classificadores:\n",
    "            modelos[str(n)] = classificador(CalibratedClassifierCV(Perceptron(max_iter=10)), \n",
    "                                            n_estimators = n, \n",
    "                                            max_features=max_features)\n",
    "        \n",
    "    else:\n",
    "        max_features = 1.0\n",
    "        \n",
    "        modelos = dict()\n",
    "        n_classificadores = np.arange(10, 110, 10)\n",
    "        for n in n_classificadores:\n",
    "            modelos[str(n)] = classificador(CalibratedClassifierCV(Perceptron(max_iter=10)), \n",
    "                                            n_estimators = n)\n",
    "        \n",
    "    return modelos\n",
    "\n",
    "# função para realizar a validação cruzada dos classificadores \n",
    "def avalia_L_modelos(L_classificadores, X, y, kfolds):\n",
    "    \n",
    "    scores_L = dict.fromkeys(L_classificadores)\n",
    "    \n",
    "    for classificador in L_classificadores:\n",
    "        print('Qtd. classificadores: ', classificador)\n",
    "        scores = dict.fromkeys(kfolds.keys())\n",
    "        for fold, (train_idx, test_idx) in kfolds.items():\n",
    "            L_classificadores[classificador].fit(X[train_idx], y[train_idx])\n",
    "            pred = L_classificadores[classificador].predict(X[test_idx])\n",
    "            scores[fold] = {'acuracia': accuracy_score(y[test_idx], pred),\n",
    "                           'modelo': L_classificadores[classificador]}\n",
    "            \n",
    "        scores_L[classificador] = scores\n",
    "    \n",
    "    return scores_L\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # criando dicionário para receber os scores\n",
    "#     scores = dict.fromkeys(kfolds.keys())\n",
    "    \n",
    "#     # loop para criar os folds\n",
    "#     for fold, (id_train, id_test) in kfolds.items():\n",
    "#         print(fold)\n",
    "#         X_train, X_test = X[id_train], X[id_test]\n",
    "#         y_train, y_test = y[id_train], y[id_test]\n",
    "        \n",
    "#         # criar dicionário para receber os scores dos folds\n",
    "#         scores_L = dict.fromkeys(L_classificadores)\n",
    "#         # loop para realizar treinamento e classificação\n",
    "#         for classificador in L_classificadores:\n",
    "#             print('Qtd. classificadores: ', classificador)\n",
    "#             L_classificadores[classificador].fit(X_train, y_train)\n",
    "#             pred = L_classificadores[classificador].predict(X_test)\n",
    "#             scores_L[classificador] = {'accuracy': accuracy_score(y_test, pred), \n",
    "#                                        'modelo': L_classificadores[classificador]}\n",
    "        \n",
    "#         # armazenar score para os L_classificadores\n",
    "#         scores[fold] = scores_L\n",
    "        \n",
    "#     return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando os L classificadores\n",
    "modelos_L = L_modelos(BaggingClassifier)\n",
    "\n",
    "# avaliando por validação cruzada\n",
    "bagging_classifier_cv = avalia_L_modelos(modelos_L, X_train, y_train, kfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('média:         ',np.mean([bagging_classifier_cv['10'][f'fold{i}']['acuracia'] for i in range(1, 6)]))\n",
    "print('desvio padrão: ', np.std([bagging_classifier_cv['10'][f'fold{i}']['acuracia'] for i in range(1, 6)], ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrando a accuracy para cada fold para todos os n classificadores\n",
    "print('10: ', [bagging_classifier_cv['10'][f'fold{i}']['acuracia'] for i in range(1, 6)])\n",
    "print('20: ',[bagging_classifier_cv['20'][f'fold{i}']['acuracia'] for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "# criando os L classificadores\n",
    "modelos_L_adaboost = L_modelos(AdaBoostClassifier)\n",
    "\n",
    "adaboost_classifier_cv = avalia_L_modelos(modelos_L_adaboost, X_train, y_train, kfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('média:         ',np.mean([adaboost_classifier_cv['10'][f'fold{i}']['acuracia'] for i in range(1, 6)]))\n",
    "print('desvio padrão: ', np.std([adaboost_classifier_cv['10'][f'fold{i}']['acuracia'] for i in range(1, 6)], ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrando a accuracy para cada fold para todos os n classificadores\n",
    "print('10: ', [adaboost_classifier_cv['10'][f'fold{i}']['acuracia'] for i in range(1, 6)])\n",
    "print('20: ',[adaboost_classifier_cv['20'][f'fold{i}']['acuracia'] for i in range(1, 6)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random subspace\n",
    "# criando os L classificadores\n",
    "modelos_L_random_subspace = L_modelos(BaggingClassifier, True)\n",
    "\n",
    "random_subspace_classifier_cv = avalia_L_modelos(modelos_L_random_subspace, X_train, y_train, kfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('média:         ',np.mean([random_subspace_classifier_cv['10'][f'fold{i}']['acuracia'] for i in range(1, 6)]))\n",
    "print('desvio padrão: ', np.std([random_subspace_classifier_cv['10'][f'fold{i}']['acuracia'] for i in range(1, 6)], ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrando a accuracy para cada fold para todos os n classificadores\n",
    "print('10: ', [random_subspace_classifier_cv['10'][f'fold{i}']['acuracia'] for i in range(1, 6)])\n",
    "print('30: ',[random_subspace_classifier_cv['30'][f'fold{i}']['acuracia'] for i in range(1, 6)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Oracle\n",
    "## Bagging\n",
    "def avalia_oracle(classificador_cv, X, y, kfolds):\n",
    "    classificador_oracles = dict.fromkeys(classificador_cv.keys())\n",
    "    for n in classificador_oracles:\n",
    "        print('Qtd. classificadores: ', n)\n",
    "        oracles = []\n",
    "        for fold, (_, test_id) in kfolds.items():\n",
    "            oracle_i = oracle.Oracle(classificador_cv[n][fold]['modelo'])\n",
    "            oracle_score = oracle_i.score(X[test_id], y[test_id])\n",
    "            oracles.append(oracle_score)\n",
    "        classificador_oracles[n] = oracles\n",
    "    return classificador_oracles\n",
    "    \n",
    "# bag_oracles = dict.fromkeys(bagging_classifier_cv.keys())\n",
    "# for n in n_classificadores:\n",
    "#     print('Qtd. classificadores: ', n)\n",
    "#     oracles = [] \n",
    "#     for fold, (_, test_id) in kfolds.items():\n",
    "#         oracle_i = oracle.Oracle(bagging_classifier_cv[str(n)][fold]['modelo'])\n",
    "# #         oracle_pred = oracle_i.predict(X_train[test_id], y_train[test_id])\n",
    "#         oracle_score = oracle_i.score(X_train[test_id], y_train[test_id])\n",
    "# #         oracles_fold[fold] = {'pred':oracle_pred, 'score': oracle_score}\n",
    "#         oracles.append(oracle_score)\n",
    "#     bag_oracles[str(n)] = oracles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_oracles = avalia_oracle(bagging_classifier_cv, X_train, y_train, kfolds)\n",
    "bag_oracles_df = pd.DataFrame(bag_oracles)\n",
    "\n",
    "adaboost_oracles = avalia_oracle(adaboost_classifier_cv, X_train, y_train, kfolds)\n",
    "adaboost_oracles_df = pd.DataFrame(adaboost_oracles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_oracles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_oracles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.boxplot(bag_oracles.values(), labels=bag_oracles.keys())\n",
    "plt.title('Boxplot: Oracle para o Bagging no conjunto de teste')\n",
    "plt.xlabel('Quantidade de classificadores')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.boxplot(adaboost_oracles.values(), labels=adaboost_oracles.keys())\n",
    "plt.title('Boxplot: Oracle para o Adaboost no conjunto de teste')\n",
    "plt.xlabel('Quantidade de classificadores')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oraculo = oracle.Oracle(random_subspace_classifier_cv['fold1']['20']['modelo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, test_idx = kfolds['fold1']\n",
    "# X2, y2 = X_train[test_idx], y_train[test_idx]\n",
    "\n",
    "# oraculo = oracle.Oracle(bagging_classifier_cv['fold1']['10']['modelo'])\n",
    "# oraculo.predict(X2, y2)\n",
    "# oraculo.score(X2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2)\n",
    "Use as mesma bases de dados e os mesmos folds da questão anterior e, para cada base: \n",
    "- use o **SGH** para gerar o pool de classificadores no conjunto de treinamento; \n",
    "\n",
    "- calcule o Oracle do pool no conjunto de teste; \n",
    "\n",
    "- verifique quantas instâncias por classe foram incorretamente classificadas; \n",
    "\n",
    "- verifique quantos hiperplanos por classe foram gerados. \n",
    "\n",
    "Analise os resultados coletados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sgh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "\n",
    "### Código para gerar os modelos SGH para as cinco bases de dados. Calcular o ORACLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
